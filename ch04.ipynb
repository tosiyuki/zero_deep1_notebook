{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter4 Learning Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# correct is 2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# ex1. if probability of 2 is highest\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "# ex2. if probability of 7 is highest\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 cross entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.01, 1, 0.01)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 mini batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49648, 42529, 29059, 50984, 42151, 57449, 21384, 44501, 49365,\n",
       "       42292])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 implementation of cross entropy error for mini batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.reshape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.reshape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size, t)])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad example\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 example of numerical diffrentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 partial differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3**2 + x1**2\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # calculate f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # calculate f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / 2*h\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.e-08, 8.e-08])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.e+00, 4.e-08])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.e-08, 0.e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9999994,  3.9999992])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99994,  3.99992])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  4.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 gradient for newral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # init Gaussian distribution\n",
    "\n",
    "    def preditct(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.preditct(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0382677  -0.12450991 -0.58598128]\n",
      " [-0.44913056 -0.06220848  0.27386217]]\n",
      "[-0.42717812 -0.13069357 -0.10511282]\n",
      "2\n",
      "0.9930272279703866\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.preditct(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "t = np.array([0, 0, 1])\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1610696   0.21665823 -0.37772783]\n",
      " [ 0.2416044   0.32498734 -0.56659175]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Implementation of Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2Layers newral network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # x:input data t: teacher data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # dummy input datas(100)\n",
    "t = np.random.rand(100, 10) # dummy correct data(100)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # calculate gradient\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 implementation of mini batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # get mini batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # calculate gradirnt\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # update parameter\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # record learning result\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09863333333333334, 0.0958\n",
      "train acc, test acc | 0.7808166666666667, 0.787\n",
      "train acc, test acc | 0.87615, 0.8794\n",
      "train acc, test acc | 0.89855, 0.9015\n",
      "train acc, test acc | 0.9066666666666666, 0.9106\n",
      "train acc, test acc | 0.91495, 0.9158\n",
      "train acc, test acc | 0.9187, 0.92\n",
      "train acc, test acc | 0.9232333333333334, 0.9252\n",
      "train acc, test acc | 0.9267666666666666, 0.9284\n",
      "train acc, test acc | 0.9303833333333333, 0.932\n",
      "train acc, test acc | 0.93355, 0.934\n",
      "train acc, test acc | 0.9364666666666667, 0.9372\n",
      "train acc, test acc | 0.93825, 0.9374\n",
      "train acc, test acc | 0.9413333333333334, 0.9411\n",
      "train acc, test acc | 0.9429333333333333, 0.9424\n",
      "train acc, test acc | 0.9445333333333333, 0.9425\n",
      "train acc, test acc | 0.9465666666666667, 0.9451\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# number of iterations per epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# hyper parameter\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # get mini batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # calculate gradirnt\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # update parameter\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # record learning result\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # calculate accuracy per epoch\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('python3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cb3e4bfe2c0f6801abca51d1496d952ff31b1ea59eb05286c604cf5196e3e5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
